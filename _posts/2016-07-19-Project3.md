---
layout: post
title: Project 3
---

This is the third project.

## Step 1: Exploring and cleaning data.
- The Iowa State legislature is considering changes in the liquor tax rates and wants a report of current liquor sales by county and projections for the rest of the year.
- We have 270,955 rows with 18 columns. 
- There are four columns with null values, 'County Number', 'County', 'Category', 'Category Name'.
- I uppercase all the string column info for easy comparison later.
- '$' are deleted so we have float types for those numbers.
- 'Date' made to datetime type.
- There are two columns 'Volume Sold (Liters)' and 'Volume Sold (Gallons)'. Liters will be used since "Bottle Volume (ml)' is in ml.
- I found some abnormal Zip Code, so I downloaded a Zip Code csv file for IOWA to check. At the end, the rows for the ZIP Codes which do not belong the Iowa were dropped.
- From the Iowa state csv info file, it has City names too. So I used that for checking my Alcohol sales data too. I created a dictionary from the official file. I wrote a function to check similar string for city names such as ARNOLD'S PARK and ARNOLDS PARK are actually the same. 
- I also created a function to check how two columns are mapped from and to, ie if values in col1 can only map to one value of col2. For example, for each for the store number, it only maps to a unique 'County Number'. Using such method, I am able to fill some of the na fields.

```python
def check_uniq_map(map_from, map_to) :
    dft = df[df[map_to].isnull()==False].groupby(by=[map_from, map_to])[[map_to]].count().unstack(level=0)
    check_uniq = []
    for col in dft.columns : 
        check_uniq.append(len(dft[col]) - dft[col].isnull().sum())
    return pd.Series(check_uniq).max() == pd.Series(check_uniq).min() 
```    
    
- 'Margin', 'Price per Bottle', 'Price per Liter' columns are created.

![]({{ site.baseurl }}/images/project3/dfhist.JPG)


## Step 2: Recreate dataframe for modelling.
- I create a new dataframe based on store info, ie each row is defined by each store and its relevant info.
- Since I have to compute many measures (sales, volume, etc) for different periods, a function was created for processing this (taking in mainly two dates, when we start and when we end). Of particular interests are 2015 Q1, 2015, and 2016 Q1.

```python
def sales_per_store_df(date_from, date_to, label) :

    # Filter by our start and end dates
    df.sort_values(by=["Store Number", "Date"], inplace=True)
    start_date = pd.Timestamp(date_from)
    end_date = pd.Timestamp(date_to)
    mask = (df['Date'] >= start_date) & (df['Date'] <= end_date)
    sales_temp = df[mask]

    # Group by store name
    sales_temp = sales_temp.groupby(by=["Store Number"], as_index=False)
    # Compute sums, means
    sales_temp = sales_temp.agg({"Sale (Dollars)": {'sum': np.sum, 'mean':np.mean},
                   "Volume Sold (Liters)": {'sum': np.sum, 'mean':np.mean},
                   "Margin": {'mean':np.mean},
                   "Price per Liter": {'mean':np.mean},
                   "Date": {'open': lambda x: len(np.unique(x))},
                   "Category Name": {'': lambda x: x.value_counts().sort_values(ascending=False).index[0]},
                   "Bottles Sold": {'sum': np.sum},
                   "Price per Bottle": {'mean':np.mean}
                   })
    # Collapse the column indices

    sales_temp.columns = [' '.join(col).strip() for col in sales_temp.columns.values]
    # Rename columns
    sales_temp = sales_temp.reindex(columns=["Store Number", "Sale (Dollars) sum", "Sale (Dollars) mean", 
                                             "Volume Sold (Liters) sum", 
                                            "Volume Sold (Liters) mean", "Margin mean", "Price per Liter mean", 
                                            "Date open", "Category Name", "Bottles Sold sum", "Price per Bottle mean"])
    # Quick check
    sales_temp.columns = ['Store Number', label + ' Sale (Dollars) sum',
           label + ' Sale (Dollars) mean', 
           label + ' Volume Sold (Liters) sum', 
           label + ' Volume Sold (Liters) mean', label + ' Margin mean',
           label + ' Price per Liter mean', label + ' days open', label + ' most fav category',
           label + ' Bottles Sold sum', label + ' Price per Bottle mean' ]
    
    sales_temp[label + ' days open'] = sales_temp[label + ' days open'].astype(int)
    sales_temp2 = df.groupby(by=["Store Number", 'Date'], as_index=False)['Sale (Dollars)'].mean()
    sales_temp2 = sales_temp2.groupby(by='Store Number')[['Sale (Dollars)']].mean()
    sales_temp2.columns = [label + ' average sales per day']
    
    return pd.merge(sales_temp, sales_temp2, how='left', left_on='Store Number', right_index=True)
```

![]({{ site.baseurl }}/images/project3/store_vs_sales_2015.JPG)

![]({{ site.baseurl }}/images/project3/city_vs_sales_2015.JPG)

![]({{ site.baseurl }}/images/project3/zip_vs_sales_2015.JPG)

- From the above, with further analysis, we can find which City and Zip Code areas consume the most and then see how those stores in those areas perform. If the sales for those stores are huge, we can introduce new stores there to grab markets.


![]({{ site.baseurl }}/images/project3/corr.JPG)

- Chart looks good but maybe not as useful.



## Step 3: Modelling.

- Plotting 2015 Q1 Sales per store vs 2015 total year as suggested.

![]({{ site.baseurl }}/images/project3/2015Q1vs2015.JPG)


- Looks very linear to me.

- Let's try all the numerical columns of 2015 Q1 for modelling 2015 full year sales.

- I have 10 features and use LassoCV for feature selection using SelectFromModel and we get only 1 column back. That is 2015 Q1 sales!

- I then use cross validation for both LinearRegression() and RidgeCV(). Scores are both 0.862.

![]({{ site.baseurl }}/images/project3/linear_score.JPG)

![]({{ site.baseurl }}/images/project3/ridge_score.JPG)

- the actual 2015 total sales number is 27,368,410 and I predict 2016 full year sales to be 27,752,689.

- Then, I tried to put in the categorical features 'City', 'County Number', 'Zip Code' and use LassoCV SelectFromModel and still got the same old 1 feature '2015 Q1 Sale (Dollars) sum'.

- Adventureously, for SelectFromModel I then tried LassoLarsCV and got 27 features out of 800 something (mostly categorical). Yet, the scores are worse with 0.754 for LinearRegression and 0.812 for RidgeCV.

- Using Q1 to predict full year is too good of a predictor. Well, usually store sales would not deviate much as time progresses especially for averages of many stores. So this somehow became recursive.

